{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1EjTdI2nuAGrzpErPoZJsiDicMw74k8Vj","timestamp":1714493103042}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9urZYrR1ox6","executionInfo":{"status":"ok","timestamp":1714493287029,"user_tz":240,"elapsed":1830,"user":{"displayName":"Henry Xie","userId":"01728094339728906749"}},"outputId":"f21dde3e-06a2-4a55-b2d6-b437d077a1a6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qx5sxMFnqu2u","executionInfo":{"status":"ok","timestamp":1714493287218,"user_tz":240,"elapsed":190,"user":{"displayName":"Henry Xie","userId":"01728094339728906749"}}},"outputs":[],"source":["# we.py\n","\n","from __future__ import print_function, division\n","import re\n","import sys\n","import numpy as np\n","import scipy.sparse\n","from sklearn.decomposition import PCA\n","if sys.version_info[0] < 3:\n","    import io\n","    open = io.open\n","else:\n","    unicode = str\n","\"\"\"\n","Tools for debiasing word embeddings\n","\n","Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n","Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai\n","2016\n","\"\"\"\n","\n","DEFAULT_NUM_WORDS = 27000\n","FILENAMES = {\"g_wiki\": \"glove.6B.300d.small.txt\",\n","             \"g_twitter\": \"glove.twitter.27B.200d.small.txt\",\n","             \"g_crawl\": \"glove.840B.300d.small.txt\",\n","             \"w2v\": \"GoogleNews-word2vec.small.txt\",\n","             \"w2v_large\":  \"GoogleNews-word2vec.txt\"}\n","\n","\n","def dedup(seq):\n","    seen = set()\n","    return [x for x in seq if not (x in seen or seen.add(x))]\n","\n","\n","def safe_word(w):\n","    # ignore words with numbers, etc.\n","    # [a-zA-Z\\.'_\\- :;\\(\\)\\]] for emoticons\n","    return (re.match(r\"^[a-z_]*$\", w) and len(w) < 20 and not re.match(r\"^_*$\", w))\n","\n","\n","def to_utf8(text, errors='strict', encoding='utf8'):\n","    \"\"\"Convert a string (unicode or bytestring in `encoding`), to bytestring in utf8.\"\"\"\n","    if isinstance(text, unicode):\n","        return text.encode('utf8')\n","    # do bytestring -> unicode -> utf8 full circle, to ensure valid utf8\n","    return unicode(text, encoding, errors=errors).encode('utf8')\n","\n","\n","class WordEmbedding:\n","    def __init__(self, fname):\n","        self.thresh = None\n","        self.max_words = None\n","        self.desc = fname\n","        print(\"*** Reading data from \" + fname)\n","        if fname.endswith(\".bin\") or fname.endswith(\".bin.gz\"):\n","            import gensim.models\n","            model =gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n","            words = model.index_to_key\n","            # words = sorted([w for w in model.vocab], key=lambda w: model.vocab[w].index)\n","            vecs = [np.array(model[w]) for w in words]\n","        else:\n","            vecs = []\n","            words = []\n","\n","            with open(fname, \"r\", encoding='utf8') as f:\n","                for line in f:\n","                    s = line.split()\n","                    v = np.array([float(x) for x in s[1:]])\n","                    if len(vecs) and vecs[-1].shape!=v.shape:\n","                        print(\"Got weird line\", line)\n","                        continue\n","    #                 v /= np.linalg.norm(v)\n","                    words.append(s[0])\n","                    vecs.append(v)\n","        self.vecs = np.array(vecs, dtype='float32')\n","        self.vecs_matrix = np.vstack(self.vecs)\n","        self.words = words\n","        self.reindex()\n","        norms = np.linalg.norm(self.vecs, axis=1)\n","        if max(norms)-min(norms) > 0.0001:\n","            self.normalize()\n","\n","    def reindex(self):\n","        self.index = {w: i for i, w in enumerate(self.words)}\n","        self.n, self.d = self.vecs.shape\n","        assert self.n == len(self.words) == len(self.index)\n","        self._neighbors = None\n","        print(self.n, \"words of dimension\", self.d, \":\", \", \".join(self.words[:4] + [\"...\"] + self.words[-4:]))\n","\n","    def v(self, word):\n","        return self.vecs[self.index[word]]\n","\n","    def diff(self, word1, word2):\n","        v = self.vecs[self.index[word1]] - self.vecs[self.index[word2]]\n","        return v/np.linalg.norm(v)\n","\n","    def normalize(self):\n","        self.desc += \", normalize\"\n","        self.vecs /= np.linalg.norm(self.vecs, axis=1)[:, np.newaxis]\n","        self.reindex()\n","\n","    def shrink(self, numwords):\n","        self.desc += \", shrink \" + str(numwords)\n","        self.filter_words(lambda w: self.index[w]<numwords)\n","\n","    def filter_words(self, test):\n","        \"\"\"\n","        Keep some words based on test, e.g. lambda x: x.lower()==x\n","        \"\"\"\n","        self.desc += \", filter\"\n","        kept_indices, words = zip(*[[i, w] for i, w in enumerate(self.words) if test(w)])\n","        self.words = list(words)\n","        self.vecs = self.vecs[kept_indices, :]\n","        self.reindex()\n","\n","    def save(self, filename):\n","        with open(filename, \"w\") as f:\n","            f.write(\"\\n\".join([w+\" \" + \" \".join([str(x) for x in v]) for w, v in zip(self.words, self.vecs)]))\n","        print(\"Wrote\", self.n, \"words to\", filename)\n","\n","    def save_w2v(self, filename, binary=True):\n","        with open(filename, 'wb') as fout:\n","            fout.write(to_utf8(\"%s %s\\n\" % self.vecs.shape))\n","            # store in sorted order: most frequent words at the top\n","            for i, word in enumerate(self.words):\n","                row = self.vecs[i]\n","                if binary:\n","                    fout.write(to_utf8(word) + b\" \" + row.tostring())\n","                else:\n","                    fout.write(to_utf8(\"%s %s\\n\" % (word, ' '.join(\"%f\" % val for val in row))))\n","\n","    def remove_directions(self, directions): #directions better be orthogonal\n","        self.desc += \", removed\"\n","        for direction in directions:\n","            self.desc += \" \"\n","            if type(direction) is np.ndarray:\n","                v = direction / np.linalg.norm(direction)\n","                self.desc += \"vector \"\n","            else:\n","                w1, w2 = direction\n","                v = self.diff(w1, w2)\n","                self.desc += w1 + \"-\" + w2\n","            self.vecs = self.vecs - self.vecs.dot(v)[:, np.newaxis].dot(v[np.newaxis, :])\n","        self.normalize()\n","\n","    def compute_neighbors_if_necessary(self, thresh, max_words):\n","        thresh = float(thresh) # dang python 2.7!\n","        if self._neighbors is not None and self.thresh == thresh and self.max_words == max_words:\n","            return\n","        print(\"Computing neighbors\")\n","        self.thresh = thresh\n","        self.max_words = max_words\n","        vecs = self.vecs[:max_words]\n","        dots = vecs.dot(vecs.T)\n","        dots = scipy.sparse.csr_matrix(dots * (dots >= 1-thresh/2))\n","        from collections import Counter\n","        rows, cols = dots.nonzero()\n","        nums = list(Counter(rows).values())\n","        print(\"Mean:\", np.mean(nums)-1)\n","        print(\"Median:\", np.median(nums)-1)\n","        rows, cols, vecs = zip(*[(i, j, vecs[i]-vecs[j]) for i, j, x in zip(rows, cols, dots.data) if i<j])\n","        self._neighbors = rows, cols, np.array([v/np.linalg.norm(v) for v in vecs])\n","\n","    def neighbors(self, word, thresh=1):\n","        dots = self.vecs.dot(self.v(word))\n","        return [self.words[i] for i, dot in enumerate(dots) if dot >= 1-thresh/2]\n","\n","    def more_words_like_these(self, words, topn=50, max_freq=100000):\n","        v = sum(self.v(w) for w in words)\n","        dots = self.vecs[:max_freq].dot(v)\n","        thresh = sorted(dots)[-topn]\n","        words = [w for w, dot in zip(self.words, dots) if dot>=thresh]\n","        return sorted(words, key=lambda w: self.v(w).dot(v))[-topn:][::-1]\n","\n","    def best_analogies_dist_thresh(self, v, thresh=1, topn=500, max_words=50000):\n","        \"\"\"Metric is cos(a-c, b-d) if |b-d|^2 < thresh, otherwise 0\n","        \"\"\"\n","        vecs, vocab = self.vecs[:max_words], self.words[:max_words]\n","        self.compute_neighbors_if_necessary(thresh, max_words)\n","        rows, cols, vecs = self._neighbors\n","        scores = vecs.dot(v/np.linalg.norm(v))\n","        pi = np.argsort(-abs(scores))\n","\n","        ans = []\n","        usedL = set()\n","        usedR = set()\n","        for i in pi:\n","            if abs(scores[i])<0.001:\n","                break\n","            row = rows[i] if scores[i] > 0 else cols[i]\n","            col = cols[i] if scores[i] > 0 else rows[i]\n","            if row in usedL or col in usedR:\n","                continue\n","            usedL.add(row)\n","            usedR.add(col)\n","            ans.append((vocab[row], vocab[col], abs(scores[i])))\n","            if len(ans)==topn:\n","                break\n","\n","        return ans\n","\n","\n","def viz(analogies):\n","    print(\"\\n\".join(str(i).rjust(4)+a[0].rjust(29) + \" | \" + a[1].ljust(29) + (str(a[2]))[:4] for i, a in enumerate(analogies)))\n","\n","\n","def text_plot_words(xs, ys, words, width = 90, height = 40, filename=None):\n","    PADDING = 10 # num chars on left and right in case words spill over\n","    res = [[' ' for i in range(width)] for j in range(height)]\n","    def rescale(nums):\n","        a = min(nums)\n","        b = max(nums)\n","        return [(x-a)/(b-a) for x in nums]\n","    print(\"x:\", (min(xs), max(xs)), \"y:\",(min(ys),max(ys)))\n","    xs = rescale(xs)\n","    ys = rescale(ys)\n","    for (x, y, word) in zip(xs, ys, words):\n","        i = int(x*(width - 1 - PADDING))\n","        j = int(y*(height-1))\n","        row = res[j]\n","        z = list(row[i2] != ' ' for i2 in range(max(i-1, 0), min(width, i + len(word) + 1)))\n","        if any(z):\n","            continue\n","        for k in range(len(word)):\n","            if i+k>=width:\n","                break\n","            row[i+k] = word[k]\n","    string = \"\\n\".join(\"\".join(r) for r in res)\n","#     return string\n","    if filename:\n","        with open(filename, \"w\", encoding=\"utf8\") as f:\n","            f.write(string)\n","        print(\"Wrote to\", filename)\n","    else:\n","        print(string)\n","\n","\n","def doPCA(pairs, embedding, num_components = 10):\n","    matrix = []\n","    for a, b in pairs:\n","        center = (embedding.v(a) + embedding.v(b))/2\n","        matrix.append(embedding.v(a) - center)\n","        matrix.append(embedding.v(b) - center)\n","    matrix = np.array(matrix)\n","    pca = PCA(n_components = num_components)\n","    pca.fit(matrix)\n","    # bar(range(num_components), pca.explained_variance_ratio_)\n","    return pca\n","\n","\n","def drop(u, v):\n","    return u - v * u.dot(v) / v.dot(v)"]},{"cell_type":"code","source":["# debias.py\n","\n","from __future__ import print_function, division\n","import json\n","import numpy as np\n","import argparse\n","import sys\n","import scipy\n","if sys.version_info[0] < 3:\n","    import io\n","    open = io.open\n","\"\"\"\n","Hard-debias embedding\n","\n","Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n","Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai\n","2016\n","\"\"\"\n","\n","def debias(E, gender_specific_words, definitional, equalize, indirect_bias_threshold):\n","    gender_components = doPCA(definitional, E).components_\n","    gender_direction = gender_components[0]\n","    removed_subspace = [gender_direction]\n","\n","    specific_set = set(gender_specific_words)\n","    for i, w in enumerate(E.words):\n","        if w not in specific_set:\n","            for direction in removed_subspace:\n","              E.vecs[i] = drop(E.vecs[i], direction)\n","    E.normalize()\n","    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n","                                                     (e1.title(), e2.title()),\n","                                                     (e1.upper(), e2.upper())]}\n","    for (a, b) in candidates:\n","        if (a in E.index and b in E.index):\n","            for direction in removed_subspace:\n","              y = drop((E.v(a) + E.v(b)) / 2, direction)\n","              z = np.sqrt(1 - np.linalg.norm(y)**2)\n","              if (E.v(a) - E.v(b)).dot(direction) < 0:\n","                  z = -z\n","              E.vecs[E.index[a]] = z * direction + y\n","              E.vecs[E.index[b]] = -z * direction + y\n","    E.normalize()"],"metadata":{"id":"fdtM0Z861DlM","executionInfo":{"status":"ok","timestamp":1714493287219,"user_tz":240,"elapsed":2,"user":{"displayName":"Henry Xie","userId":"01728094339728906749"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","embedding_filename = '/content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/GoogleNews-vectors-negative300-SLIM.bin.gz'\n","definitional_filename = '/content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/definitional_pairs.json'\n","gendered_words_filename = '/content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/gender_specific_full.json'\n","equalize_filename = '/content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/equalize_pairs.json'\n","debiased_filename = '/content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/word2vec_original_debiased_slimmed.json'\n","\n","with open(definitional_filename, \"r\") as f:\n","    defs = json.load(f)\n","print(\"definitional\", defs)\n","\n","with open(equalize_filename, \"r\") as f:\n","    equalize_pairs = json.load(f)\n","\n","with open(gendered_words_filename, \"r\") as f:\n","    gender_specific_words = json.load(f)\n","print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])\n","\n","E = WordEmbedding(embedding_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZJyXT2z1Ut0","outputId":"afcacc38-c067-42dd-828e-1f8e64ffc88b","executionInfo":{"status":"ok","timestamp":1714493304056,"user_tz":240,"elapsed":16839,"user":{"displayName":"Henry Xie","userId":"01728094339728906749"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["definitional [['woman', 'man'], ['girl', 'boy'], ['she', 'he'], ['mother', 'father'], ['daughter', 'son'], ['gal', 'guy'], ['female', 'male'], ['her', 'his'], ['herself', 'himself'], ['Mary', 'John']]\n","gender specific 1441 ['he', 'his', 'He', 'her', 'she', 'him', 'She', 'man', 'women', 'men']\n","*** Reading data from /content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/GoogleNews-vectors-negative300-SLIM.bin.gz\n","299567 words of dimension 300 : in, for, that, is, ..., StoreFront_e, KUNDI, tricorne, RAFFAELE\n"]}]},{"cell_type":"code","source":["print(\"Debiasing...\")\n","debias(E, gender_specific_words, defs, equalize_pairs, 0.6)\n","\n","print(\"Saving to file...\")\n","if embedding_filename[-4:] == debiased_filename[-4:] == \".bin\":\n","    E.save_w2v(debiased_filename)\n","else:\n","    E.save(debiased_filename)\n","\n","print(\"\\n\\nDone!\\n\")"],"metadata":{"id":"wSCURhmCB1He","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714493388127,"user_tz":240,"elapsed":84076,"user":{"displayName":"Henry Xie","userId":"01728094339728906749"}},"outputId":"328d368f-487d-49ea-89ea-1a3b741ac4d0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Debiasing...\n","299567 words of dimension 300 : in, for, that, is, ..., StoreFront_e, KUNDI, tricorne, RAFFAELE\n","299567 words of dimension 300 : in, for, that, is, ..., StoreFront_e, KUNDI, tricorne, RAFFAELE\n","Saving to file...\n","Wrote 299567 words to /content/drive/MyDrive/Courses/Sophomore Spring Courses/CS226R/Debiasing Algorithm Data/word2vec_original_debiased_slimmed.json\n","\n","\n","Done!\n","\n"]}]}]}